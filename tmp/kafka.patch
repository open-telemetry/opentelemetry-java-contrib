diff --git a/jmx-metrics/docs/target-systems/kafka.md b/jmx-metrics/docs/target-systems/kafka.md
index 5c0dcbe..9642020 100644
--- a/jmx-metrics/docs/target-systems/kafka.md
+++ b/jmx-metrics/docs/target-systems/kafka.md
@@ -7,31 +7,75 @@ These metrics are sourced from Kafka's exposed Yammer metrics for each instance:
 
 ### Broker Metrics
 
-* Name: `kafka.messages.in`
-* Description: Number of messages in per second
-* Unit: `1`
-* Instrument Type: LongValueObserver
+* Name: `kafka.message.count`
+* Description: The number of messages received by the broker
+* Unit: `messages`
+* Attributes: `topic`
+* Instrument Type: LongCounterObserver
+
+* Name: `kafka.request.count`
+* Description: The number of requests received by the broker
+* Unit: `requests`
+* Attributes: `topic`, `type`
+* Instrument Type: LongCounterObserver
+
+* Name: `kafka.request.failed`
+* Description: The number of requests to the broker resulting in a failure
+* Unit: `requests`
+* Attributes: `topic`, `type`
+* Instrument Type: LongCounterObserver
+
+* Name: `kafka.request.time.total`
+* Description: The total time the broker has taken to service requests
+* Unit: `ms`
+* Attributes: `type`
+* Instrument Type: LongCounterObserver
+
+* Name: `kafka.request.time.50p`
+* Description: The 50th percentile time the broker has taken to service requests
+* Unit: `ms`
+* Attributes: `type`
+* Instrument Type: DoubleValueObserver
 
-* Name: `kafka.bytes.in`
-* Description: Bytes in per second from clients
+* Name: `kafka.request.time.99p`
+* Description: The 99th percentile time the broker has taken to service requests
+* Unit: `ms`
+* Attributes: `type`
+* Instrument Type: DoubleValueObserver
+
+* Name: `kafka.network.io`
+* Description: The bytes received or sent by the broker
 * Unit: `by`
+* Attributes: `topic`, `state`
+* Instrument Type: LongCounterObserver
+
+* Name: `kafka.purgatory.size`
+* Description: The number of requests waiting in purgatory
+* Unit: `requests`
+* Attributes: `type`
 * Instrument Type: LongValueObserver
 
-* Name: `kafka.bytes.out`
-* Description: Bytes out per second to clients
-* Unit: `by`
+* Name: `kafka.partition.count`
+* Description: The number of partitions on the broker
+* Unit: `partitions`
 * Instrument Type: LongValueObserver
 
-* Name: `kafka.isr.shrinks`
-* Description: In-sync replica shrinks per second
-* Unit: `1`
+* Name: `kafka.partition.offline`
+* Description: The number of partitions offline
+* Unit: `partitions`
 * Instrument Type: LongValueObserver
 
-* Name: `kafka.isr.expands`
-* Description: In-sync replica expands per second
-* Unit: `1`
+* Name: `kafka.partition.under_replicated`
+* Description: The number of under replicated partitions
+* Unit: `partitions`
 * Instrument Type: LongValueObserver
 
+* Name: `kafka.isr.operation.count`
+* Description: The number of in-sync replica shrink and expand operations
+* Unit: `operations`
+* Attributes: `operation`
+* Instrument Type: LongCounterObserver
+
 * Name: `kafka.max.lag`
 * Description: Max lag in messages between follower and leader replicas
 * Unit: `1`
@@ -42,16 +86,6 @@ These metrics are sourced from Kafka's exposed Yammer metrics for each instance:
 * Unit: `1`
 * Instrument Type: LongValueObserver
 
-* Name: `kafka.partitions.offline.count`
-* Description: Number of partitions without an active leader
-* Unit: `1`
-* Instrument Type: LongValueObserver
-
-* Name: `kafka.partitions.underreplicated.count`
-* Description: Number of under replicated partitions
-* Unit: `1`
-* Instrument Type: LongValueObserver
-
 * Name: `kafka.leader.election.rate`
 * Description: Leader election rate - non-zero indicates broker failures
 * Unit: `1`
@@ -67,51 +101,6 @@ These metrics are sourced from Kafka's exposed Yammer metrics for each instance:
 * Unit: `1`
 * Instrument Type: LongValueObserver
 
-* Name: `kafka.fetch.consumer.total.time.count`
-* Description: Fetch consumer request count
-* Unit: `1`
-* Instrument Type: LongSumObserver
-
-* Name: `kafka.fetch.consumer.total.time.median`
-* Description: Fetch consumer request time - 50th percentile
-* Unit: `ms`
-* Instrument Type: DoubleValueObserver
-
-* Name: `kafka.fetch.consumer.total.time.99p`
-* Description: Fetch consumer request time - 99th percentile
-* Unit: `ms`
-* Instrument Type: DoubleValueObserver
-
-* Name: `kafka.fetch.follower.total.time.count`
-* Description: Fetch follower request count
-* Unit: `1`
-* Instrument Type: LongSumObserver
-
-* Name: `kafka.fetch.follower.total.time.median`
-* Description: Fetch follower request time - 50th percentile
-* Unit: `ms`
-* Instrument Type: DoubleValueObserver
-
-* Name: `kafka.fetch.follower.total.time.99p`
-* Description: Fetch follower request time - 99th percentile
-* Unit: `ms`
-* Instrument Type: DoubleValueObserver
-
-* Name: `kafka.produce.total.time.count`
-* Description: Produce request count
-* Unit: `1`
-* Instrument Type: LongSumObserver
-
-* Name: `kafka.produce.total.time.median`
-* Description: Produce request time - 50th percentile
-* Unit: `ms`
-* Instrument Type: DoubleValueObserver
-
-* Name: `kafka.produce.total.time.99p`
-* Description: Produce request time - 99th percentile
-* Unit: `ms`
-* Instrument Type: DoubleValueObserver
-
 ### Log metrics
 
 * Name: `kafka.logs.flush.time.count`
diff --git a/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/AbstractIntegrationTest.java b/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/AbstractIntegrationTest.java
index 710650c..662c57c 100644
--- a/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/AbstractIntegrationTest.java
+++ b/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/AbstractIntegrationTest.java
@@ -112,8 +112,8 @@ public abstract class AbstractIntegrationTest {
     otlpServer.reset();
   }
 
-  @SafeVarargs
-  protected final void waitAndAssertMetrics(Consumer<Metric>... assertions) {
+  @SuppressWarnings("varargs")
+  protected final void waitAndAssertMetrics(Iterable<Consumer<Metric>> assertions) {
     await()
         .atMost(Duration.ofSeconds(30))
         .untilAsserted(
@@ -145,6 +145,12 @@ public abstract class AbstractIntegrationTest {
             });
   }
 
+  @SafeVarargs
+  @SuppressWarnings("varargs")
+  protected final void waitAndAssertMetrics(Consumer<Metric>... assertions) {
+    waitAndAssertMetrics(Arrays.asList(assertions));
+  }
+
   protected void assertGauge(Metric metric, String name, String description, String unit) {
     assertThat(metric.getName()).isEqualTo(name);
     assertThat(metric.getDescription()).isEqualTo(description);
diff --git a/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/target_systems/KafkaIntegrationTest.java b/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/target_systems/KafkaIntegrationTest.java
index 5137cb1..00eaf6b 100644
--- a/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/target_systems/KafkaIntegrationTest.java
+++ b/jmx-metrics/src/integrationTest/java/io/opentelemetry/contrib/jmxmetrics/target_systems/KafkaIntegrationTest.java
@@ -6,12 +6,14 @@
 package io.opentelemetry.contrib.jmxmetrics.target_systems;
 
 import static org.assertj.core.api.Assertions.assertThat;
+import static org.assertj.core.api.Assertions.entry;
 
 import io.opentelemetry.contrib.jmxmetrics.AbstractIntegrationTest;
 import io.opentelemetry.proto.metrics.v1.Metric;
 import io.opentelemetry.proto.metrics.v1.NumberDataPoint;
 import java.io.IOException;
 import java.time.Duration;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
@@ -28,6 +30,29 @@ import org.testcontainers.lifecycle.Startable;
 import org.testcontainers.utility.MountableFile;
 
 abstract class KafkaIntegrationTest extends AbstractIntegrationTest {
+  Startable createTopics =
+      new Startable() {
+        @Override
+        public void start() {
+          try {
+            kafka.execInContainer(
+                "sh",
+                "-c",
+                "unset JMX_PORT; for i in `seq 3`; do kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test-topic-$i; done");
+          } catch (IOException | InterruptedException e) {
+            throw new AssertionError(e);
+          }
+        }
+
+        @Override
+        public void stop() {}
+
+        @Override
+        public Set<Startable> getDependencies() {
+          return Collections.singleton(kafka);
+        }
+      };
+
   protected KafkaIntegrationTest(String configName) {
     super(/* configFromStdin= */ false, configName);
   }
@@ -53,28 +78,101 @@ abstract class KafkaIntegrationTest extends AbstractIntegrationTest {
           .waitingFor(Wait.forListeningPort())
           .dependsOn(zookeeper);
 
-  Startable createTopics =
-      new Startable() {
-        @Override
-        public void start() {
-          try {
-            kafka.execInContainer(
-                "sh",
-                "-c",
-                "unset JMX_PORT; for i in `seq 3`; do kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test-topic-$i; done");
-          } catch (IOException | InterruptedException e) {
-            throw new AssertionError(e);
-          }
-        }
-
-        @Override
-        public void stop() {}
-
-        @Override
-        public Set<Startable> getDependencies() {
-          return Collections.singleton(kafka);
-        }
-      };
+  List<Consumer<Metric>> kafkaBrokerAssertions() {
+    return Arrays.asList(
+        metric ->
+            assertSumWithAttributes(
+                metric,
+                "kafka.request.time.total",
+                "The total time the broker has taken to service requests",
+                "ms",
+                attrs -> attrs.containsOnly(entry("type", "Produce")),
+                attrs -> attrs.containsOnly(entry("type", "Fetch"))
+            ),
+        metric ->
+            assertGaugeWithAttributes(
+                metric,
+                "kafka.request.time.50p",
+                "The 50th percentile time the broker has taken to service requests",
+                "ms",
+                attrs -> attrs.containsOnly(entry("type", "Produce")),
+                attrs -> attrs.containsOnly(entry("type", "Fetch"))
+            ),
+        metric ->
+            assertGaugeWithAttributes(
+                metric,
+                "kafka.request.time.99p",
+                "The 99th percentile time the broker has taken to service requests",
+                "ms",
+                attrs -> attrs.containsOnly(entry("type", "Produce")),
+                attrs -> attrs.containsOnly(entry("type", "Fetch"))
+            ),
+        metric ->
+            assertGaugeWithAttributes(
+                metric,
+                "kafka.purgatory.size",
+                "The number of requests waiting in purgatory",
+                "requests",
+                attrs -> attrs.containsOnly(entry("type", "Produce")),
+                attrs -> attrs.containsOnly(entry("type", "Fetch"))
+            ),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.partition.count",
+                "The total number of partitions on the broker",
+                "partitions"
+            ),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.partition.offline",
+                "The number of partitions offline",
+                "partitions"
+            ),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.partition.under_replicated",
+                "The number of under replicated partitions",
+                "partitions"
+            ),
+        metric ->
+            assertSumWithAttributes(
+                metric,
+                "kafka.isr.operation.count",
+                "The number of in-sync replica shrink and expand operations",
+                "operations",
+                attrs -> attrs.containsOnly(entry("operation", "Shrink")),
+                attrs -> attrs.containsOnly(entry("operation", "Expand"))
+            ),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.controller.active.count",
+                "controller is active on broker",
+                "controllers"
+            ),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.leader.election.rate",
+                "leader election rate - increasing indicates broker failures",
+                "elections"),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.max.lag",
+                "max lag in messages between follower and leader replicas",
+                "messages"),
+        metric ->
+            assertGauge(
+                metric,
+                "kafka.unclean.election.rate",
+                "unclean leader election rate - non-zero indicates broker failures",
+                "elections")
+        );
+  }
 
   static class KafkaBrokerTargetIntegrationTest extends KafkaIntegrationTest {
     KafkaBrokerTargetIntegrationTest() {
@@ -83,97 +181,7 @@ abstract class KafkaIntegrationTest extends AbstractIntegrationTest {
 
     @Test
     void endToEnd() {
-      waitAndAssertMetrics(
-          metric -> assertGauge(metric, "kafka.bytes.in", "bytes in per second from clients", "by"),
-          metric -> assertGauge(metric, "kafka.bytes.out", "bytes out per second to clients", "by"),
-          metric ->
-              assertGauge(
-                  metric, "kafka.controller.active.count", "controller is active on broker", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.consumer.total.time.99p",
-                  "fetch consumer request time - 99th percentile",
-                  "ms"),
-          metric ->
-              assertSum(
-                  metric,
-                  "kafka.fetch.consumer.total.time.count",
-                  "fetch consumer request count",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.consumer.total.time.median",
-                  "fetch consumer request time - 50th percentile",
-                  "ms"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.follower.total.time.99p",
-                  "fetch follower request time - 99th percentile",
-                  "ms"),
-          metric ->
-              assertSum(
-                  metric,
-                  "kafka.fetch.follower.total.time.count",
-                  "fetch follower request count",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.follower.total.time.median",
-                  "fetch follower request time - 50th percentile",
-                  "ms"),
-          metric ->
-              assertGauge(metric, "kafka.isr.shrinks", "in-sync replica shrinks per second", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.leader.election.rate",
-                  "leader election rate - non-zero indicates broker failures",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.max.lag",
-                  "max lag in messages between follower and leader replicas",
-                  "1"),
-          metric ->
-              assertGauge(metric, "kafka.messages.in", "number of messages in per second", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.partitions.offline.count",
-                  "number of partitions without an active leader",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.partitions.underreplicated.count",
-                  "number of under replicated partitions",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.produce.total.time.99p",
-                  "produce request time - 99th percentile",
-                  "ms"),
-          metric ->
-              assertSum(metric, "kafka.produce.total.time.count", "produce request count", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.produce.total.time.median",
-                  "produce request time - 50th percentile",
-                  "ms"),
-          metric -> assertGauge(metric, "kafka.request.queue", "size of the request queue", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.unclean.election.rate",
-                  "unclean leader election rate - non-zero indicates broker failures",
-                  "1"));
+      waitAndAssertMetrics(kafkaBrokerAssertions());
     }
   }
 
@@ -370,134 +378,57 @@ abstract class KafkaIntegrationTest extends AbstractIntegrationTest {
               "G1 Old Gen",
               "G1 Survivor Space",
               "Metaspace");
-      waitAndAssertMetrics(
-          metric -> assertGauge(metric, "jvm.classes.loaded", "number of loaded classes", "1"),
-          metric ->
-              assertTypedSum(
-                  metric,
-                  "jvm.gc.collections.count",
-                  "total number of collections that have occurred",
-                  "1",
-                  Arrays.asList("G1 Young Generation", "G1 Old Generation")),
-          metric ->
-              assertTypedSum(
-                  metric,
-                  "jvm.gc.collections.elapsed",
-                  "the approximate accumulated collection elapsed time in milliseconds",
-                  "ms",
-                  Arrays.asList("G1 Young Generation", "G1 Old Generation")),
-          metric -> assertGauge(metric, "jvm.memory.heap.committed", "current heap usage", "by"),
-          metric -> assertGauge(metric, "jvm.memory.heap.init", "current heap usage", "by"),
-          metric -> assertGauge(metric, "jvm.memory.heap.max", "current heap usage", "by"),
-          metric -> assertGauge(metric, "jvm.memory.heap.used", "current heap usage", "by"),
-          metric ->
-              assertGauge(metric, "jvm.memory.nonheap.committed", "current non-heap usage", "by"),
-          metric -> assertGauge(metric, "jvm.memory.nonheap.init", "current non-heap usage", "by"),
-          metric -> assertGauge(metric, "jvm.memory.nonheap.max", "current non-heap usage", "by"),
-          metric -> assertGauge(metric, "jvm.memory.nonheap.used", "current non-heap usage", "by"),
-          metric ->
-              assertTypedGauge(
-                  metric, "jvm.memory.pool.committed", "current memory pool usage", "by", gcLabels),
-          metric ->
-              assertTypedGauge(
-                  metric, "jvm.memory.pool.init", "current memory pool usage", "by", gcLabels),
-          metric ->
-              assertTypedGauge(
-                  metric, "jvm.memory.pool.max", "current memory pool usage", "by", gcLabels),
-          metric ->
-              assertTypedGauge(
-                  metric, "jvm.memory.pool.used", "current memory pool usage", "by", gcLabels),
-          metric -> assertGauge(metric, "jvm.threads.count", "number of threads", "1"),
-          metric -> assertGauge(metric, "kafka.bytes.in", "bytes in per second from clients", "by"),
-          metric -> assertGauge(metric, "kafka.bytes.out", "bytes out per second to clients", "by"),
-          metric ->
-              assertGauge(
-                  metric, "kafka.controller.active.count", "controller is active on broker", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.consumer.total.time.99p",
-                  "fetch consumer request time - 99th percentile",
-                  "ms"),
-          metric ->
-              assertSum(
-                  metric,
-                  "kafka.fetch.consumer.total.time.count",
-                  "fetch consumer request count",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.consumer.total.time.median",
-                  "fetch consumer request time - 50th percentile",
-                  "ms"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.follower.total.time.99p",
-                  "fetch follower request time - 99th percentile",
-                  "ms"),
-          metric ->
-              assertSum(
-                  metric,
-                  "kafka.fetch.follower.total.time.count",
-                  "fetch follower request count",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.fetch.follower.total.time.median",
-                  "fetch follower request time - 50th percentile",
-                  "ms"),
-          metric ->
-              assertGauge(metric, "kafka.isr.shrinks", "in-sync replica shrinks per second", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.leader.election.rate",
-                  "leader election rate - non-zero indicates broker failures",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.max.lag",
-                  "max lag in messages between follower and leader replicas",
-                  "1"),
-          metric ->
-              assertGauge(metric, "kafka.messages.in", "number of messages in per second", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.partitions.offline.count",
-                  "number of partitions without an active leader",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.partitions.underreplicated.count",
-                  "number of under replicated partitions",
-                  "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.produce.total.time.99p",
-                  "produce request time - 99th percentile",
-                  "ms"),
-          metric ->
-              assertSum(metric, "kafka.produce.total.time.count", "produce request count", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.produce.total.time.median",
-                  "produce request time - 50th percentile",
-                  "ms"),
-          metric -> assertGauge(metric, "kafka.request.queue", "size of the request queue", "1"),
-          metric ->
-              assertGauge(
-                  metric,
-                  "kafka.unclean.election.rate",
-                  "unclean leader election rate - non-zero indicates broker failures",
-                  "1"));
+      List<Consumer<Metric>> assertions = new ArrayList<>(kafkaBrokerAssertions());
+      assertions.addAll(
+          Arrays.asList(
+              metric -> assertGauge(metric, "jvm.classes.loaded", "number of loaded classes", "1"),
+              metric ->
+                  assertTypedSum(
+                      metric,
+                      "jvm.gc.collections.count",
+                      "total number of collections that have occurred",
+                      "1",
+                      Arrays.asList("G1 Young Generation", "G1 Old Generation")),
+              metric ->
+                  assertTypedSum(
+                      metric,
+                      "jvm.gc.collections.elapsed",
+                      "the approximate accumulated collection elapsed time in milliseconds",
+                      "ms",
+                      Arrays.asList("G1 Young Generation", "G1 Old Generation")),
+              metric ->
+                  assertGauge(metric, "jvm.memory.heap.committed", "current heap usage", "by"),
+              metric -> assertGauge(metric, "jvm.memory.heap.init", "current heap usage", "by"),
+              metric -> assertGauge(metric, "jvm.memory.heap.max", "current heap usage", "by"),
+              metric -> assertGauge(metric, "jvm.memory.heap.used", "current heap usage", "by"),
+              metric ->
+                  assertGauge(
+                      metric, "jvm.memory.nonheap.committed", "current non-heap usage", "by"),
+              metric ->
+                  assertGauge(metric, "jvm.memory.nonheap.init", "current non-heap usage", "by"),
+              metric ->
+                  assertGauge(metric, "jvm.memory.nonheap.max", "current non-heap usage", "by"),
+              metric ->
+                  assertGauge(metric, "jvm.memory.nonheap.used", "current non-heap usage", "by"),
+              metric ->
+                  assertTypedGauge(
+                      metric,
+                      "jvm.memory.pool.committed",
+                      "current memory pool usage",
+                      "by",
+                      gcLabels),
+              metric ->
+                  assertTypedGauge(
+                      metric, "jvm.memory.pool.init", "current memory pool usage", "by", gcLabels),
+              metric ->
+                  assertTypedGauge(
+                      metric, "jvm.memory.pool.max", "current memory pool usage", "by", gcLabels),
+              metric ->
+                  assertTypedGauge(
+                      metric, "jvm.memory.pool.used", "current memory pool usage", "by", gcLabels),
+              metric -> assertGauge(metric, "jvm.threads.count", "number of threads", "1")));
+
+      waitAndAssertMetrics(assertions);
     }
   }
 
diff --git a/jmx-metrics/src/main/resources/target-systems/kafka.groovy b/jmx-metrics/src/main/resources/target-systems/kafka.groovy
index 928df09..1c66e98 100644
--- a/jmx-metrics/src/main/resources/target-systems/kafka.groovy
+++ b/jmx-metrics/src/main/resources/target-systems/kafka.groovy
@@ -14,81 +14,178 @@
  * limitations under the License.
  */
 
-def messagesInPerSec = otel.mbean("kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec")
-otel.instrument(messagesInPerSec, "kafka.messages.in", "number of messages in per second",
-        "1", "Count", otel.&longValueCallback)
+def messagesInPerSec = otel.mbean("kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=*")
+otel.instrument(messagesInPerSec,
+  "kafka.message.count",
+  "The number of messages received by the broker",
+  "messages",
+  [
+    "topic" : { mbean -> mbean.name().getKeyProperty("topic") },
+  ],
+  "Count", otel.&longCounterCallback)
+
+def requests = otel.mbeans(["kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec,topic=*",
+                           "kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec,topic=*"])
+otel.instrument(requests,
+  "kafka.request.count",
+  "The number of requests received by the broker",
+  "requests",
+  [
+    "topic" : { mbean -> mbean.name().getKeyProperty("topic") },
+    "type" : { mbean -> switch(mbean.name().getKeyProperty("name")) {
+      case "TotalProduceRequestsPerSec":
+        return "Produce"
+        break
+      case "TotalFetchRequestsPerSec":
+        return "Fetch"
+        break
+      }
+    },
+  ],
+  "Count", otel.&longCounterCallback)
+
+def failedRequests = otel.mbeans(["kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec,topic=*",
+                           "kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec,topic=*"])
+otel.instrument(failedRequests,
+  "kafka.request.failed",
+  "The number of requests to the broker resulting in a failure",
+  "requests",
+  [
+    "topic" : { mbean -> mbean.name().getKeyProperty("topic") },
+    "type" : { mbean -> switch(mbean.name().getKeyProperty("name")) {
+      case "FailedProduceRequestsPerSec":
+        return "Produce"
+        break
+      case "FailedFetchRequestsPerSec":
+        return "Fetch"
+        break
+      }
+    },
+  ],
+  "Count", otel.&longCounterCallback)
+
+def requestTime = otel.mbean("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=*")
+otel.instrument(requestTime,
+  "kafka.request.time.total",
+  "The total time the broker has taken to service requests",
+  "ms",
+  [
+    "type" : { mbean -> mbean.name().getKeyProperty("request") },
+  ],
+  "Count", otel.&longCounterCallback)
+otel.instrument(requestTime,
+  "kafka.request.time.50p",
+  "The 50th percentile time the broker has taken to service requests",
+  "ms",
+  [
+    "type" : { mbean -> mbean.name().getKeyProperty("request") },
+  ],
+  "50thPercentile", otel.&doubleValueCallback)
+otel.instrument(requestTime,
+  "kafka.request.time.99p",
+  "The 99th percentile time the broker has taken to service requests",
+  "ms",
+  [
+    "type" : { mbean -> mbean.name().getKeyProperty("request") },
+  ],
+  "99thPercentile", otel.&doubleValueCallback)
+
+
+
+def network = otel.mbeans(["kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=*",
+                          "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=*"])
+otel.instrument(network,
+  "kafka.network.io",
+  "The bytes received or sent by the broker",
+  "by",
+  [
+    "topic" : { mbean -> mbean.name().getKeyProperty("topic") },
+    "state" : { mbean -> switch(mbean.name().getKeyProperty("name")) {
+      case "BytesInPerSec":
+        return "In"
+        break
+      case "BytesOutPerSec":
+        return "Out"
+        break
+      }
+    },
+  ],
+  "Count", otel.&longCounterCallback)
+
+def purgatorySize = otel.mbean("kafka.server:type=DelayedOperationPurgatory,name=PurgatorySize,delayedOperation=*")
+otel.instrument(purgatorySize,
+  "kafka.purgatory.size",
+  "The number of requests waiting in purgatory",
+  "requests",
+  [
+    "type" : { mbean -> mbean.name().getKeyProperty("delayedOperation") },
+  ],
+  "Value", otel.&longValueCallback)
+
+def partitionCount = otel.mbean("kafka.server:type=ReplicaManager,name=PartitionCount")
+otel.instrument(partitionCount,
+  "kafka.partition.count",
+  "The number of partitions on the broker",
+  "partitions",
+  "Value", otel.&longValueCallback)
+
+def partitionOffline = otel.mbean("kafka.controller:type=KafkaController,name=OfflinePartitionsCount")
+otel.instrument(partitionOffline,
+  "kafka.partition.offline",
+  "The number of partitions offline",
+  "partitions",
+  "Value", otel.&longValueCallback)
+
+def partitionUnderReplicated = otel.mbean("kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions")
+otel.instrument(partitionUnderReplicated,
+  "kafka.partition.under_replicated",
+  "The number of under replicated partitions",
+  "partitions",
+  "Value", otel.&longValueCallback)
+
+def isrOperations = otel.mbeans(["kafka.server:type=ReplicaManager,name=IsrShrinksPerSec",
+                                "kafka.server:type=ReplicaManager,name=IsrExpandsPerSec"])
+otel.instrument(isrOperations,
+  "kafka.isr.operation.count",
+  "The number of in-sync replica shrink and expand operations",
+  "operations",
+  [
+    "operation" : { mbean -> switch(mbean.name().getKeyProperty("name")) {
+      case "IsrShrinksPerSec":
+        return "Shrink"
+        break
+      case "IsrExpandsPerSec":
+        return "Expand"
+        break
+      }
+    },
+  ],
+  "Count", otel.&longCounterCallback)
 
-def bytesInPerSec = otel.mbean("kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec")
-otel.instrument(bytesInPerSec, "kafka.bytes.in", "bytes in per second from clients",
-        "by", "Count", otel.&longValueCallback)
-
-def bytesOutPerSec = otel.mbean("kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec")
-otel.instrument(bytesOutPerSec, "kafka.bytes.out", "bytes out per second to clients",
-        "by", "Count", otel.&longValueCallback)
-
-def isrShrinksPerSec = otel.mbean("kafka.server:type=ReplicaManager,name=IsrShrinksPerSec")
-otel.instrument(isrShrinksPerSec, "kafka.isr.shrinks", "in-sync replica shrinks per second",
-        "1", "Count", otel.&longValueCallback)
-
-def isrExpandsPerSec = otel.mbean("kafka.server:type=ReplicaManager,name=IsrExpandsPerSec")
-otel.instrument(isrExpandsPerSec, "kafka.isr.expands", "in-sync replica expands per second",
-        "1", "Count", otel.&longValueCallback)
 
 def maxLag = otel.mbean("kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica")
 otel.instrument(maxLag, "kafka.max.lag", "max lag in messages between follower and leader replicas",
-        "1", "Value", otel.&longValueCallback)
+        "messages", "Value", otel.&longValueCallback)
 
 def activeControllerCount = otel.mbean("kafka.controller:type=KafkaController,name=ActiveControllerCount")
 otel.instrument(activeControllerCount, "kafka.controller.active.count", "controller is active on broker",
-        "1", "Value", otel.&longValueCallback)
-
-def offlinePartitionsCount = otel.mbean("kafka.controller:type=KafkaController,name=OfflinePartitionsCount")
-otel.instrument(offlinePartitionsCount, "kafka.partitions.offline.count", "number of partitions without an active leader",
-        "1", "Value", otel.&longValueCallback)
-
-def underReplicatedPartitionsCount = otel.mbean("kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions")
-otel.instrument(underReplicatedPartitionsCount, "kafka.partitions.underreplicated.count", "number of under replicated partitions",
-        "1", "Value", otel.&longValueCallback)
+        "controllers", "Value", otel.&longValueCallback)
 
 def leaderElectionRate = otel.mbean("kafka.controller:type=ControllerStats,name=LeaderElectionRateAndTimeMs")
-otel.instrument(leaderElectionRate, "kafka.leader.election.rate", "leader election rate - non-zero indicates broker failures",
-        "1", "Count", otel.&longValueCallback)
+otel.instrument(leaderElectionRate, "kafka.leader.election.rate", "leader election rate - increasing indicates broker failures",
+        "elections", "Count", otel.&longCounterCallback)
 
 def uncleanLeaderElections = otel.mbean("kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec")
-otel.instrument(uncleanLeaderElections, "kafka.unclean.election.rate", "unclean leader election rate - non-zero indicates broker failures",
-        "1", "Count", otel.&longValueCallback)
+otel.instrument(uncleanLeaderElections, "kafka.unclean.election.rate", "unclean leader election rate - increasing indicates broker failures",
+        "elections", "Count", otel.&longCounterCallback)
 
 def requestQueueSize = otel.mbean("kafka.network:type=RequestChannel,name=RequestQueueSize")
 otel.instrument(requestQueueSize, "kafka.request.queue", "size of the request queue",
-        "1", "Value", otel.&longValueCallback)
-
-def fetchConsumer = otel.mbean("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer")
-otel.instrument(fetchConsumer, "kafka.fetch.consumer.total.time.count", "fetch consumer request count",
-        "1", "Count", otel.&longCounterCallback)
-otel.instrument(fetchConsumer, "kafka.fetch.consumer.total.time.median", "fetch consumer request time - 50th percentile",
-        "ms", "50thPercentile", otel.&doubleValueCallback)
-otel.instrument(fetchConsumer, "kafka.fetch.consumer.total.time.99p", "fetch consumer request time - 99th percentile",
-        "ms", "99thPercentile", otel.&doubleValueCallback)
-
-def fetchFollower = otel.mbean("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower")
-otel.instrument(fetchFollower, "kafka.fetch.follower.total.time.count", "fetch follower request count",
-        "1", "Count", otel.&longCounterCallback)
-otel.instrument(fetchFollower, "kafka.fetch.follower.total.time.median", "fetch follower request time - 50th percentile",
-        "ms", "50thPercentile", otel.&doubleValueCallback)
-otel.instrument(fetchFollower, "kafka.fetch.follower.total.time.99p", "fetch follower request time - 99th percentile",
-        "ms", "99thPercentile", otel.&doubleValueCallback)
-
-def produce = otel.mbean("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce")
-otel.instrument(produce, "kafka.produce.total.time.count", "produce request count",
-        "1", "Count", otel.&longCounterCallback)
-otel.instrument(produce, "kafka.produce.total.time.median", "produce request time - 50th percentile",
-        "ms", "50thPercentile", otel.&doubleValueCallback)
-otel.instrument(produce, "kafka.produce.total.time.99p", "produce request time - 99th percentile",
-        "ms", "99thPercentile", otel.&doubleValueCallback)
+        "requests", "Value", otel.&longValueCallback)
 
 def logFlushRate = otel.mbean("kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs")
 otel.instrument(logFlushRate, "kafka.logs.flush.time.count", "log flush count",
-        "1", "Count", otel.&longCounterCallback)
+        "ms", "Count", otel.&longCounterCallback)
 otel.instrument(logFlushRate, "kafka.logs.flush.time.median", "log flush time - 50th percentile",
         "ms", "50thPercentile", otel.&doubleValueCallback)
 otel.instrument(logFlushRate, "kafka.logs.flush.time.99p", "log flush time - 99th percentile",
